simplerobotparser
=================

A python script to fetch and parse robots.txt files for web-crawlers.

A simple(r) robots.txt parser.
It implements Crawl-Delay and Request-Rate directives, and you can easily
expand it to do more. If you are interested, look for the comment:
"special functions -- you may want to extend this part"

This implementation follow the guidelines of 
http://www.w3.org/TR/html4/appendix/notes.html#h-B.4.1.1
http://www.robotstxt.org/
http://en.wikipedia.org/wiki/Robots.txt
and more.

***
This work is licensed under the Creative Commons 
Attribution-NonCommercial-ShareAlike 3.0 Unported License. To view a copy of
this license, visit http://creativecommons.org/licenses/by-nc-sa/3.0/.

You can share, copy, distribute, modify this script as long as you attribute
the work to Balint Vekerdy, you do not use it for commercial purposes, 
and you make it available for others under the same conditions.
***
    
