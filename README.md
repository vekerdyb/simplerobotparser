simplerobotparser
=================

A python script to fetch and parse robots.txt files for web-crawlers.

A simple(r) robots.txt parser.
It implements Crawl-Delay and Request-Rate directives, and you can easily
expand it to do more. If you are interested, look for the comment:
"special functions -- you may want to extend this part"

This implementation follow the guidelines of 
http://www.w3.org/TR/html4/appendix/notes.html#h-B.4.1.1
http://www.robotstxt.org/
http://en.wikipedia.org/wiki/Robots.txt
and more.

***
Author: Balint Vekredy
This work is licensed under the GNU GPLv3 license.
For details please see:
http://www.gnu.org/licenses/quick-guide-gplv3.html
***
    
